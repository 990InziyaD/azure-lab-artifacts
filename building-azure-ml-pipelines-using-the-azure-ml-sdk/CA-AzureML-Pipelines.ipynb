{"cells":[{"cell_type":"markdown","source":["# Building Azure ML Pipelines using the Azure Machine Learning SDK"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["The Azure Machine Learning SDK allows data scientists and AI developers to interact with the Azure Machine Learning services within any Python enviornment. This provides many benefits such as managing datasets, training models using cloud resources, and deploying trained models as web services.\r\n","\r\n","\r\n","In this notebook, you will follow along the process of using the Azure ML SDK to build a pipeline for training and model.\r\n","\r\n","Note: To execute the code in each cell, click on the cell and press SHIFT + ENTER.\r\n","\r\n"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["## Log in to Workspace"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["To login to the workspace with the Azure ML Python SDK, you will need to authenticate again with Azure. When you run this cell for the first time, you are prompted to authenticate with Azure by clicking on a link and inputting a security code into a web page.\r\n","\r\n","This block of code imports the azureml.core package which is used for interacting with Azure Machine Learning:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import azureml.core\n","from azureml.core import Workspace\n","\n","# Load the workspace from the saved config file\n","ws = Workspace.from_config()\n","print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598448890874},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["## Default Datastore\r\n","Datastores enable Azure ML users to connect data in almost any Azure Storage service to their Azure ML Workspace. The datastore becomes an abstraction layer for connecting to the various types of Azure storage.\r\n","This lab uses the default datastore attached to a storage account created by default when provisioning the Azure ML Workspace:\r\n"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["#Set Default Datastore\r\n","datastore = ws.get_default_datastore()\r\n","\r\n","print('The default datastore has been saved to a variable.')"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598448893726}}},{"cell_type":"markdown","source":["## Select Compute\r\n","A compute cluster has already been created at the beginning of this lab. This cluster will be used for processing the tasks during each pipeline step:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Select the compute cluster target\r\n","from azureml.core.compute import ComputeTarget\r\n","\r\n","cpu_cluster = ComputeTarget(workspace=ws, name='automl-compute')\r\n","\r\n","print(\"Found compute cluster!\")\r\n"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598448896596}}},{"cell_type":"markdown","source":["## Building an Azure ML Pipeline\r\n","\r\n","Azure ML Pipelines split up machine learning workflows into different steps. This workflow allows multiple users to collaborate on a single machine learning workflow by making changes to just one step. It can also save costs by using cheaper computing resources for different steps.\r\n","\r\n","In this example, you will break up the traditional workflow for training a model and build a pipeline with the following steps for training an Iris classification model:\r\n","\r\n","- Ingest Iris data from a URL\r\n","- Preprocess Iris data and split into test and training samples\r\n","- Train the model using the preprocessed data\r\n","- Evaluate the model and determine the accuracy\r\n","- Deploy the model as a web service\r\n","\r\n","Splitting up the machine learning workflow into different pipeline steps allows the workflow to scale with more massive datasets during the model's lifecycle. It also allows for multiple members of a team to manage separate parts of the workflow.\r\n","\r\n","\r\n"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"markdown","source":["### Creating the Source Directories\n","Each ML pipeline step will have it's own Python script to execute to perform the desired actions. The location for each script file and any files it depends on is called a source directory.  It's best practice to use separate folders for each source directory because a snapshot is taken of the source directory for each step. Using a different source directory for each pipeline step reduces the size of each snapshot. Any changes made to the files in each step's source directory can trigger a re-upload of the snapshot, causing that step to be rerun. \n","\n","The source directory folder structure will look like this:\n","```\n","data_dependency_run_ingest\n","  └ ingest.py\n","data_dependency_run_preprocess\n","  └ preprocess.py\n","data_dependency_run_train\n","  └ train.py\n","data_dependency_run_evaluate\n","  └ evaluate.py\n","data_dependency_run_deploy\n","  └ score.py\n","  └ deploy.py\n","```\n","\n","Run the cell block below to create the directories:\n"],"metadata":{}},{"cell_type":"code","source":["import os\n","\n","# Create the source directory for each pipeline step\n","source_directory_ingest = 'data_dependency_run_ingest'\n","source_directory_preprocess = 'data_dependency_run_preprocess'\n","source_directory_train = 'data_dependency_run_train'\n","source_directory_evaluate = 'data_dependency_run_evaluate'\n","source_directory_deploy = 'data_dependency_run_deploy'\n","\n","\n","\n","if not os.path.exists(source_directory_ingest):\n","    os.makedirs(source_directory_ingest)\n","if not os.path.exists(source_directory_preprocess):\n","    os.makedirs(source_directory_preprocess)\n","if not os.path.exists(source_directory_train ):\n","    os.makedirs(source_directory_train)\n","if not os.path.exists(source_directory_evaluate):\n","    os.makedirs(source_directory_evaluate)\n","if not os.path.exists(source_directory_deploy):\n","    os.makedirs(source_directory_deploy)\n","    \n","print('The source directories have been created.')"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598448902096}}},{"cell_type":"markdown","source":["### Creating Scripts in the Source Directories\r\n","\r\n","Each pipeline step's scripts will need to be created and placed in their respective source directory folder. Read the summary of each script and run the cell block to make each script.\r\n","\r\n","Each script contains arguments that are used to pass in the directory information between each step. "],"metadata":{}},{"cell_type":"markdown","source":["#### Ingest Script\r\n","\r\n","The ingestion step takes input for a URL and a directory to store the data. It downloads the data from the URL and saves it to a folder on the datastore:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["%%writefile $source_directory_ingest/ingest.py\n","\n","import os\n","import urllib.request\n","import argparse\n","\n","# Define arguments\n","parser = argparse.ArgumentParser(description='Iris Data Ingestion')\n","parser.add_argument('--iris_data_dir', type=str, help='Directory to store Iris Data')\n","parser.add_argument('--urls', type=str, help='Data URL to ingest')\n","args = parser.parse_args()\n","\n","\n","\n","# Get arguments from parser\n","iris_data_dir = args.iris_data_dir\n","urls = args.urls\n","\n","\n","if not os.path.exists(iris_data_dir):\n","    os.makedirs(iris_data_dir)\n","\n","\n","# Download data from URL\n","print(\"Downloading data from URL Arguments\")\n","urllib.request.urlretrieve(urls, \"{}/iris.csv\".format(iris_data_dir))\n","\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":["#### Preprocess Script\r\n","\r\n","After the data is ingested and stored on a directory on the datastore, the preprocess step takes the iris data from the previous step and splits the data into separate train and test sets. This is the typical pattern for training a machine learning model. The train and test sets are then stored in separate folders on the datastore:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["%%writefile $source_directory_preprocess/preprocess.py\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import glob\n","import os\n","import argparse\n","import pickle\n","\n","# Define arguments\n","parser = argparse.ArgumentParser(description='Preprocessing')\n","parser.add_argument('--train_dir', type=str, help='Directory to output the processed training data')\n","parser.add_argument('--iris_data_dir', type=str, help='Directory to store iris data')\n","parser.add_argument('--test_dir', type=str, help='Directory to output the processed test data')\n","\n","\n","args = parser.parse_args()\n","\n","# Get arguments from parser\n","iris_data_dir = args.iris_data_dir\n","train_dir = args.train_dir\n","test_dir = args.test_dir\n","\n","\n","\n","# Process data and split into train and test models\n","path = iris_data_dir\n","all_files = glob.glob(os.path.join(path, \"*.csv\"))\n","\n","names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n","dataset = pd.concat((pd.read_csv(f, names=names) for f in all_files))\n","\n","array = dataset.values\n","X = array[:,0:4]\n","y = array[:,4]\n","X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n","\n","# Make train and test directories if they don't exist\n","if not os.path.exists(train_dir):\n","    os.makedirs(train_dir)\n","\n","if not os.path.exists(test_dir):\n","    os.makedirs(test_dir)\n","\n","# Output processed data to their respective folders\n","with open(test_dir + '/X_test.sav', 'wb') as f:\n","    pickle.dump(X_test, f)\n","with open(test_dir + '/Y_test.sav', 'wb') as f:\n","    pickle.dump(Y_test, f)\n","with open(train_dir + '/X_train.sav', 'wb') as f:\n","    pickle.dump(X_train, f)\n","with open(train_dir + '/Y_train.sav', 'wb') as f:\n","    pickle.dump(Y_train, f)\n","    \n","    "],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":["#### Train Script\r\n","\r\n","The training step takes the preprocessed data and trains the model to fit the dataset. The training script saves the model to a directory:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["%%writefile $source_directory_train/train.py\n","\n","import os\n","from sklearn.svm import SVC\n","import pickle\n","import argparse\n","\n","\n","# Define arguments\n","parser = argparse.ArgumentParser(description='Train')\n","parser.add_argument('--train_dir', type=str, help='Directory to output the processed training data')\n","parser.add_argument('--output_dir', type=str, help='Directory to store output raw data')\n","\n","args = parser.parse_args()\n","\n","# Get arguments from parser\n","output_dir = args.output_dir\n","train_dir = args.train_dir\n","\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","\n","# load the model from the training directory\n","loaded_X_train = pickle.load(open(train_dir + '/X_train.sav', 'rb'))\n","loaded_Y_train = pickle.load(open(train_dir + '/Y_train.sav', 'rb'))\n","\n","# Fit the model with training dataset\n","model = SVC(gamma='auto')\n","model.fit(loaded_X_train, loaded_Y_train)\n","\n","# Output model to directory\n","with open(output_dir + '/model.pt', 'wb') as f:\n","    pickle.dump(model, f)\n"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":["#### Evaluation Script\r\n","\r\n","The next step is to evaluate the model and determine accuracy. The evaluation step tests the model against the test data, and an accuracy score is determined. The script then outputs the accuracy to a file:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["%%writefile $source_directory_evaluate/evaluate.py\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import SVC\n","import pickle\n","import argparse\n","\n","# Define arguments\n","parser = argparse.ArgumentParser(description='Evaluate')\n","parser.add_argument('--model_dir', type=str, help='Directory of the model')\n","parser.add_argument('--test_dir', type=str, help='Directory to output the processed test data')\n","parser.add_argument('--accuracy_dir', type=str, help='Directory to store output raw data')\n","\n","args = parser.parse_args()\n","\n","# Get arguments from parser\n","model_dir = args.model_dir\n","test_dir = args.test_dir\n","accuracy_dir = args.accuracy_dir\n","\n","# load the model and test datasets from their directories\n","loaded_model = pickle.load(open(model_dir + '/model.pt', 'rb'))\n","loaded_validx = pickle.load(open(test_dir + '/X_test.sav', 'rb'))\n","loaded_validy = pickle.load(open(test_dir + '/Y_test.sav', 'rb'))\n","\n","\n","# Evaluate predictions and output to file\n","predictions = loaded_model.predict(loaded_validx)\n","print(accuracy_score(loaded_validy, predictions))\n","accuracy = accuracy_score(loaded_validy, predictions)\n","\n","if not os.path.exists(accuracy_dir):\n","    os.makedirs(accuracy_dir)\n","\n","with open(accuracy_dir + '/accuracy_file', 'wb') as f:\n","    pickle.dump(accuracy, f)\n"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":["#### Deploy Script\r\n","\r\n","The deploy step takes the newly trained model and deploys it as a web service endpoint:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["%%writefile $source_directory_deploy/deploy.py\n","\n","import os\n","from sklearn.metrics import accuracy_score\n","\n","import pickle\n","from azureml.core.webservice import Webservice\n","from azureml.core.model import InferenceConfig\n","from azureml.core.environment import Environment\n","from azureml.core import Workspace\n","from azureml.core.model import Model\n","from azureml.core.run import Run\n","from azureml.core.conda_dependencies import CondaDependencies\n","from azureml.core.webservice import AciWebservice\n","from azureml.exceptions import WebserviceException\n","import argparse\n","\n","# Create function for registering the model\n","def register_model(output_dir, model_name, accuracy, test_dir, workspace):\n","    '''\n","    Registers a new model\n","    '''\n","    model = Model.register(\n","        model_path = model_dir + '/model.pt',\n","        model_name = 'iris-classification-pipeline',\n","        tags = {\n","            'accuracy': accuracy, \n","            'test_data': test_dir\n","        },\n","        description='Object recognition classifier',\n","        workspace=workspace)\n","    return model\n","\n","# Define arguments\n","parser = argparse.ArgumentParser(description='Deploy arg parser')\n","parser.add_argument('--test_dir', type=str, help='Directory where testing data is stored')\n","parser.add_argument('--model_dir', type=str, help='File storing the evaluation accuracy')\n","parser.add_argument('--accuracy_dir', type=str, help='File storing the evaluation accuracy')\n","\n","args = parser.parse_args()\n","\n","# Get run context\n","run = Run.get_context()\n","workspace = run.experiment.workspace\n","\n","\n","# Get arguments from parser\n","test_dir = args.test_dir\n","accuracy_dir = args.accuracy_dir\n","model_dir = args.model_dir\n","\n","if not os.path.exists(model_dir):\n","    os.makedirs(model_dir)\n","\n","\n","# Get environment install required packages\n","env = Environment('iris-env')\n","\n","# Register environment to re-use later\n","env.register(workspace = workspace)\n","\n","# Define model and service names\n","service_name = 'iris-classification-service'\n","model_name = 'iris-classification-pipeline'\n","\n","\n","\n","# Read Accuracy\n","accuracy = pickle.load(open(accuracy_dir + '/accuracy_file', 'rb'))\n","\n","# Set up Environment\n","myenv = Environment.get(workspace=workspace, name=\"iris-env\", version=\"1\")\n","cd = CondaDependencies.create(pip_packages=['azureml-dataprep[pandas,fuse]>=1.1.14', 'azureml-defaults'], conda_packages = ['scikit-learn==0.20.3'])\n","myenv.python.conda_dependencies = cd\n","\n","# Register model if accuracy is higher or if test dataset has changed\n","new_model = False\n","try:\n","    model = Model(workspace, model_name)\n","    prev_accuracy = model.tags['accuracy']\n","    prev_test_dir = model.tags['test_data']\n","    if prev_test_dir != test_dir or prev_accuracy >= accuracy:\n","        model = register_model(model_dir, model_name, accuracy, test_dir, workspace)\n","        new_model = True\n","except WebserviceException:\n","    print('Model does not exist yet')\n","    model = register_model(model_dir, model_name, accuracy, test_dir, workspace)\n","    new_model = True\n","\n","# Deploy new webservice if new model was registered\n","if new_model:\n","    # Create inference config\n","    inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n","\n","    # Deploy model\n","    aci_config = AciWebservice.deploy_configuration(\n","        cpu_cores = 2, \n","        memory_gb = 4, \n","        tags = {'model': 'iris', 'method': 'sklearn'}, \n","        description='Iris classifier')\n","\n","    try:\n","        service = Webservice(workspace, name=service_name)\n","        if service:\n","            service.delete()\n","    except WebserviceException as e:\n","        print()\n","\n","    service = Model.deploy(workspace, service_name, [model], inference_config, aci_config)\n","    service.wait_for_deployment(True)\n","else:\n","    service = Webservice(workspace, name=service_name)\n","\n","# Output scoring url to file\n","print(service.scoring_uri)\n","with open(model_dir + '/scoring_uri.txt', 'w+') as f:\n","    f.write(service.scoring_uri)\n"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":["#### Score Script\r\n","\r\n","The scoring script runs when deploying the web service. The `init` method contains the logic for retrieving the registered model. The `run` method contains logic which gets invoked when calling the web service. The example takes the model and performs a prediction against the data that gets sent to the web service endpoint:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["%%writefile $source_directory_deploy/score.py\n","import json\n","import numpy as np\n","import os\n","import pickle\n","from sklearn.svm import SVC\n","\n","\n","def init():\n","    global model\n","    # Get registered model\n","    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pt')\n","    model = pickle.load(open(model_path, 'rb'))\n","\n","def run(raw_data):\n","    data = np.array(json.loads(raw_data)['data'])\n","    # make prediction\n","    prediction = model.predict([data])\n","    # Output prediction\n","    return prediction.tolist()\n"],"outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":["### Passing Data Between Pipeline Steps\r\n","\r\n","A pipeline can take input and output data. This data can already exist from a dataset or be output data from a previous pipeline step called a PipelineData object.\r\n","\r\n","The first step in the pipeline will be the ingestion step, which downloads the Iris CSV dataset and stores it in a directory on the default datastore. The Iris CSV directory location needs to be passed on to the preprocessing step so it can perform its tasks. \r\n","\r\n","The default datastore also needs to be referenced in the PipelineData object using a data reference. This reference is a  pointer to the datastore path and is used during a run.\r\n","\r\n","Create a PipelineData object for the Iris data directory:\r\n","\r\n"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["\n","from azureml.pipeline.core import PipelineData\n","from azureml.data.data_reference import DataReference\n","\n","# Get datastore reference\n","datastore_reference = DataReference(datastore, mode='mount')\n","\n","# Create Pipeline Data\n","iris_data_dir = PipelineData(\n","    name='iris_data_dir', \n","    pipeline_output_name='iris_data_dir',\n","    datastore=datastore_reference.datastore,\n","    output_mode='mount',\n","    is_directory=True)\n","\n","print('The iris data PipelineObject has been created.')"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598448929360}}},{"cell_type":"markdown","source":["Each pipeline step will need to pass data between them. Define the additional PipelineData objects for the rest of the pipeline workflow:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Create Pipeline Data for remaining steps\r\n","train_dir = PipelineData(\r\n","    name='train_dir', \r\n","    pipeline_output_name='train_dir',\r\n","    datastore=datastore_reference.datastore,\r\n","    output_mode='mount',\r\n","    is_directory=True)\r\n","\r\n","output_dir = PipelineData(\r\n","    name='output_dir', \r\n","    pipeline_output_name='outputdir',\r\n","    datastore=datastore_reference.datastore,\r\n","    output_mode='mount',\r\n","    is_directory=True)\r\n","\r\n","accuracy_dir = PipelineData(\r\n","    name='accuracy_dir', \r\n","    pipeline_output_name='accuracydir',\r\n","    datastore=datastore_reference.datastore,\r\n","    output_mode='mount',\r\n","    is_directory=True)\r\n","\r\n","model_dir = PipelineData(\r\n","    name='model_dir', \r\n","    pipeline_output_name='modeldir',\r\n","    datastore=datastore_reference.datastore,\r\n","    output_mode='mount',\r\n","    is_directory=True)\r\n","\r\n","test_dir = PipelineData(\r\n","    name='test_dir', \r\n","    pipeline_output_name='test_dir',\r\n","    datastore=datastore_reference.datastore,\r\n","    output_mode='mount',\r\n","    is_directory=True)\r\n","\r\n","print('The remaining PipelineObjects have been created.')"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598448931456}}},{"cell_type":"markdown","source":["### Set up RunConfiguration\r\n","\r\n","The RunConfiguration object contains the information for submitting a training run in the experiment. For this run, the Conda dependencies require the Scikit-Learn package. This ML package will then be accessible during the experiment run:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["\n","from azureml.core.runconfig import RunConfiguration\n","from azureml.core.environment import CondaDependencies\n","\n","\n","# Configure the conda dependancies for the Run\n","conda_dep = CondaDependencies()\n","conda_dep.add_conda_package(\"scikit-learn==0.20.3\")\n","run_config = RunConfiguration(conda_dependencies=conda_dep)\n","\n","print('Run configuration has been created.')"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598448934441},"jupyter":{"outputs_hidden":false,"source_hidden":false}}},{"cell_type":"markdown","source":["### Defining the Pipeline Steps\r\n","\r\n","After the PipelineDataObjects have been defined, the pipeline steps can be created. There are many built-in pipeline steps available in the Azure ML SDK. For a list of more steps, check out the [pipeline step documentation](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py). For now, the PythonScriptStep is used to execute our python scripts. \r\n","\r\n","The PythonScriptStep consists of the name of the script to run and any arguments to pass through. The source directory is also defined in the PythonScriptStep.  This source directory is the local directory created earlier in the lab and is where the `ingest.py` file is located.\r\n","\r\n","A step in the pipeline can take input data and create output data. In this case, the ingestion step is taking input from the default datastore and creating output for the Iris data directory. Then the iris data directory is passed into the preprocessing step as an input. This linking of inputs and outputs creates an implicit dependency and automatically tells Azure ML which order to run the steps. You could use the [run_after](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.builder.pipelinestep?view=azure-ml-py#run-after-step-) construct to declare the order of the steps, but since there is already a data dependency between the steps, this is not necessary:\r\n"],"metadata":{}},{"cell_type":"code","source":["import os\r\n","from azureml.pipeline.steps import PythonScriptStep\r\n","\r\n","# The URL for the Iris data that will be ingested in the first step of the pipeline\r\n","url =\"https://raw.githubusercontent.com/cloudacademy/azure-lab-artifacts/master/building-azure-ml-pipelines-using-the-azure-ml-sdk/iris.csv\"\r\n","        \r\n","\r\n","# Pipeline Steps\r\n","ingestion_step = PythonScriptStep(\r\n","    script_name='ingest.py',\r\n","    arguments=['--iris_data_dir', iris_data_dir, '--urls', url],\r\n","    inputs=[datastore_reference],\r\n","    outputs=[iris_data_dir],\r\n","    compute_target=cpu_cluster,\r\n","    source_directory=source_directory_ingest,\r\n","    runconfig=run_config,\r\n","    allow_reuse=True\r\n",")\r\n","\r\n","\r\n","preprocess_step = PythonScriptStep(\r\n","    script_name='preprocess.py',\r\n","    arguments=['--iris_data_dir', iris_data_dir, '--train_dir', train_dir,'--test_dir', test_dir],\r\n","    inputs=[iris_data_dir],\r\n","    outputs=[train_dir, test_dir],\r\n","    compute_target=cpu_cluster,\r\n","    source_directory=source_directory_preprocess,\r\n","    runconfig=run_config,\r\n","    allow_reuse=True\r\n",")\r\n","\r\n","print('The ingestion and preprocess pipelines have been created.')"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598448938671}}},{"cell_type":"markdown","source":["Use an EstimatorStep for the training step of the pipeline. The EstimatorStep enables an estimator, which makes it easier to train models. In this case, the SKLearn estimator is used, which includes the scikit-learn learning framework:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from azureml.pipeline.steps import EstimatorStep\r\n","from azureml.train.sklearn import SKLearn\r\n","\r\n","\r\n","# Create estimator and estimator pipeline step\r\n","estimator = SKLearn(\r\n","    source_directory=source_directory_train,\r\n","    entry_script='train.py',\r\n","    compute_target=cpu_cluster\r\n",")\r\n","\r\n","train_step = EstimatorStep(\r\n","    estimator=estimator,\r\n","    estimator_entry_script_arguments=['--train_dir', train_dir, '--output_dir', model_dir],\r\n","    inputs=[train_dir],\r\n","    compute_target=cpu_cluster,\r\n","    outputs=[model_dir],\r\n","    allow_reuse=False\r\n",")\r\n","print('The estimator pipeline has been created.')"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598448947802}}},{"cell_type":"markdown","source":["Create the remaining pipeline steps for evaluating and deploying the model:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Create the evaluate and deploy pipeline steps\r\n","\r\n","evaluate_step = PythonScriptStep(\r\n","    script_name='evaluate.py',\r\n","    arguments=['--model_dir', model_dir,'--test_dir', test_dir, '--accuracy_dir',  accuracy_dir],\r\n","    inputs=[test_dir,model_dir],\r\n","    outputs=[accuracy_dir],\r\n","    compute_target=cpu_cluster,\r\n","    source_directory=source_directory_evaluate,\r\n","    runconfig=run_config,\r\n","    allow_reuse=True\r\n",")\r\n","\r\n","deploy_step = PythonScriptStep(\r\n","    script_name='deploy.py',\r\n","    arguments=['--model_dir', model_dir, '--accuracy_dir',  accuracy_dir,'--test_dir', test_dir],\r\n","    inputs=[test_dir,accuracy_dir,model_dir],\r\n","    outputs=[output_dir],\r\n","    compute_target=cpu_cluster,\r\n","    source_directory=source_directory_deploy,\r\n","    runconfig=run_config,\r\n","    allow_reuse=True\r\n",")\r\n","\r\n","print('The evaluate and deploy pipelines have been created.')"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1598448950375}}},{"cell_type":"markdown","source":["### Run Pipeline\r\n","\r\n","Submit the pipeline to initiate the run; this may take up to 30 minutes for the pipeline to complete:"],"metadata":{}},{"cell_type":"code","source":["from azureml.pipeline.core import Pipeline\n","from azureml.core import Experiment\n","\n","\n","# Submit the pipeline\n","print('Submitting pipeline ...')\n","pipeline = Pipeline(workspace=ws, steps=[ingestion_step,preprocess_step, train_step, evaluate_step, deploy_step])\n","pipeline_run = Experiment(ws, 'iris_pipeline').submit(pipeline)"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598448960909}}},{"cell_type":"markdown","source":["To view the progress of the pipeline, either click the link to the Azure Machine Learning Portal generated by the code cell above or run the azureml widget to view the pipeline's status run through the Jupyter Notebook:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Show run details\n","from azureml.widgets import RunDetails\n","RunDetails(pipeline_run).show()"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598448969342}}},{"cell_type":"markdown","source":["## Test the Endpoint\r\n","\r\n","Once the web service is deployed, it can be tested by sending a series of sepal and petal measurements to the URI. The web service will take the data, run a model prediction against it, and return the classification prediction:"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["import urllib.request, urllib.error # urllib.request and urllib.error for Python 3.X\n","import json\n","from azureml.core.webservice import Webservice\n","\n","\n","# Iris petal and sepal measurements\n","rawdata = {\"data\": [\n","                6.7, \n","                3.0, \n","                5.2, \n","                2.3\n","            ]\n","}\n","\n","# Get the URL of the web service\n","service = Webservice(workspace=ws, name='iris-classification-service')\n","url = service.scoring_uri\n","\n","# Send data to web service\n","body = str.encode(json.dumps(rawdata))\n","\n","headers = {'Content-Type':'application/json', 'Authorization':('Bearer ')}\n","req = urllib.request.Request(url, body, headers)\n","\n","try:\n","    response = urllib.request.urlopen(req)\n","    result = response.read()\n","    print(result)\n","\n","except urllib.error.HTTPError as error: \n","    print(\"The request failed with status code: \" + str(error.code))\n","    print(error.info())\n"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1598387052885},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}}}],"metadata":{"kernel_info":{"name":"python3-azureml"},"kernelspec":{"name":"python3-azureml","language":"python","display_name":"Python 3.6 - AzureML"},"language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}